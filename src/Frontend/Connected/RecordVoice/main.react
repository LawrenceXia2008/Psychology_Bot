import React from "react";

import Button from "direct-core/UI/Button";

import { connect } from "react-redux";

import { bindActionCreators } from "redux";

import * as actions from "actions";

var processingTime = 0;

class AutoCatchAudio extends React.Component {
  allClips = []
  chunks = []
  noSoundFor = 0
  startTalk = false
  overAllProcessTime = 0
  interval = 12 // 2 , 3 , 4 , 5
  state = {
    clips: []
  }

  getAudio = ref => this.audio = ref

  constructor( props ){
    super( props );
  }

  componentDidMount(){
    navigator.mediaDevices.getUserMedia({
      audio: true
    }).then( stream => {
      this.mediaRecorder = new MediaRecorder( stream )
      this.perpareStreamAnalysis( stream.clone() )
      // this let me know the sound speaker makes every 1 ms
      this.mediaRecorder.ondataavailable = this.dataProcess
      if( this.props.autoStart ){
        this.startRecord();
      }
      // this gives me a blob
    }).catch( err => {
      console.log( err );
    })
    
  }




  perpareStreamAnalysis = ( stream ) => {
    const audioCtx = new AudioContext(undefined,undefined,this.props.sampleingRate);
    const mediaStreamSource = audioCtx.createMediaStreamSource( stream.clone() );
    var processor = audioCtx.createScriptProcessor();

    processor.onaudioprocess = ( event ) => {
      const buf = event.inputBuffer.getChannelData( 0 );
      const length = buf.length;
      var sum = 0;
      var x;
      processingTime++;
      this.overAllProcessTime++;
      // Do a root-mean-square on the samples: sum up the squares...
      for( let i = 0; i < length ; i++ ){
        x = buf[i];
        sum += x * x;
      }

      const rms = Math.sqrt( sum / length );
      processor.volume += rms;
    }

    processor.volume = 0;
    // no effect

    this.meter = processor;

    mediaStreamSource.connect( this.meter );
    processor.connect( audioCtx.destination );
    setInterval( this.streamAnalysis , this.interval );
  }

  /*
  called every 10ms
  */
  streamAnalysis = () => {
    if( !this.overAllProcessTime ){
      return;
    }
    const overAllLevel = this.meter.volume / this.overAllProcessTime;
    //console.log( overAllLevel , this.startTalk , this.noSoundFor );
    if( !this.startTalk ){
      if( overAllLevel >= 0.05 ){
        this.startTalk = true;
      }
      this.overAllProcessTime = this.meter.volume = 0;
      return;
    }

    if( overAllLevel <= 0.05 ){
      this.noSoundFor += this.interval;
      if( this.noSoundFor > 1000 ){
        if( this.startTalk ){
          if( this.mediaRecorder.state !== "inactive" ){
            this.stopRecord();
          }
          this.startTalk = false;
          this.noSoundFor = 0;
        }
      }
    } else {
      this.noSoundFor = 0;
    }
    this.overAllProcessTime = this.meter.volume = 0;
  }

  /*
  this will be called every 600ms with a start block , a data block or a end block
  when merging , it must be start-data-data-....-data-end
  so merge function must be excuted here
  and mergeRecords is just a request
  so , if requested , and last block is not a end block,
  this function will immediately call this.stopReord
  which implements the stop of the record and set a signal
  which indicates last block is triggered as stop block
  after merge , all flags are reset
  if no merge request , this function just collect every new block

  in a complete system, a merge request will be fired when user haven't
  talk for 1000 ms
  */
  dataProcess = ( ev ) => {
    console.log( this.startTalk )
    if( this.startTalk ){
      this.chunks.push( ev.data );
    }
    if( this.isFirstBlock ){
      if( !this.isLastBlockStop ){
        this.chunks.push( ev.data );
        this.isFirstBlock = false;
      }
    }
    if( this.isLastBlockStop ){
      this.isLastBlockStop = false;
      this.chunks.push( ev.data );
      var blob = new Blob( this.chunks , {
        type: "audio/wav"
      });
      var audioURL = URL.createObjectURL( blob );
      this.allClips.push( audioURL );
      this.chunks = [];
      this.props.getClip({ blob , url: audioURL });
      if( this.props.show ){
        this.audio.src = audioURL;
      }
      this.noSoundFor = 0;
    }
  }

  startRecord = () => {
    this.mediaRecorder.start();
    this.handle = setInterval( () => this.mediaRecorder.requestData() , 500 );
    this.isFirstBlock = true;
  }

  stopRecord = () => {
    this.isLastBlockStop = true;
    clearInterval( this.handle );
    this.mediaRecorder.stop();
    this.startRecord();
  }


  updateAllClip = () => {
    this.setState({
      clips: this.allClips
    });
  }

  render(){
    if( !this.props.show ){
      return null;
    }
    return (
      <React.Fragment>
        <Button
          onClick={this.startRecord}
          text="begin"
        />
        <Button
          onClick={this.stopRecord}
          text="终止"
        />
        <Button
          onClick={this.updateAllClip}
          text="更新所有"
        />
        <div>
          <span>Last audio</span>
          <audio
            controls
            ref={this.getAudio}
          />
          {
            this.state.clips.map( clip => (
              <audio
                key={clip}
                controls
                src={clip}
              />
            ))
          }
        </div>
      </React.Fragment>
    );
  }
};

export default connect(
  state => ({

  }),
  dispatch => bindActionCreators( actions , dispatch )
)(AutoCatchAudio);
